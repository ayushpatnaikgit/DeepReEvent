{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from src.data import _load_readmission_dataset, _load_simData\n",
    "from src.helper_functions import _prepare_rt_tensors, _prepare_dataloaders\n",
    "from src.models import *\n",
    "from src.losses import *    \n",
    "from src.training import *\n",
    "from src.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayush/DeepReEvent/src/data.py:47: FutureWarning: The provided callable <built-in function max> is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  data['max_time'] = data.groupby('id')['t.stop'].transform(max)\n"
     ]
    }
   ],
   "source": [
    "x, t, e, d = _load_readmission_dataset(sequential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "readmission_tensor = _prepare_rt_tensors(x, t, e, d)\n",
    "locals().update(readmission_tensor) # create variables from dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader = _prepare_dataloaders(readmission_tensor, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# model and training parameters\n",
    "\n",
    "input_size = len(x[0]) \n",
    "output_size = int(max(t))\n",
    "hidden_size = 8   # Number of units in the RNN layer\n",
    "\n",
    "model = SimpleLSTM(input_size, hidden_size, output_size, 2, 0.1)\n",
    "\n",
    "# Instantiate the model\n",
    "\n",
    "num_epochs = 10000\n",
    "patience = 3  # Number of epochs to wait for improvement before stopping\n",
    "best_val_loss = float('inf')\n",
    "wait = 0\n",
    "loss_function = recurrent_terminal_loss\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),  lr=1e-4, weight_decay=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0   Training loss: 5.8839   Validation loss: 5.6622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10   Training loss: 5.7795   Validation loss: 5.5669\n",
      "Epoch: 20   Training loss: 5.6569   Validation loss: 5.4793\n",
      "Epoch: 30   Training loss: 5.5834   Validation loss: 5.3979\n",
      "Epoch: 40   Training loss: 5.5035   Validation loss: 5.3194\n",
      "Epoch: 50   Training loss: 5.4012   Validation loss: 5.2430\n",
      "Epoch: 60   Training loss: 5.3176   Validation loss: 5.1664\n",
      "Epoch: 70   Training loss: 5.2344   Validation loss: 5.0883\n",
      "Epoch: 80   Training loss: 5.1366   Validation loss: 5.0086\n",
      "Epoch: 90   Training loss: 5.0462   Validation loss: 4.9257\n",
      "Epoch: 100   Training loss: 4.9762   Validation loss: 4.8381\n",
      "Epoch: 110   Training loss: 4.8446   Validation loss: 4.7436\n",
      "Epoch: 120   Training loss: 4.7577   Validation loss: 4.6406\n",
      "Epoch: 130   Training loss: 4.6301   Validation loss: 4.5323\n",
      "Epoch: 140   Training loss: 4.5123   Validation loss: 4.4172\n",
      "Epoch: 150   Training loss: 4.3960   Validation loss: 4.3038\n",
      "Epoch: 160   Training loss: 4.2634   Validation loss: 4.1967\n",
      "Epoch: 170   Training loss: 4.2018   Validation loss: 4.1014\n",
      "Epoch: 180   Training loss: 4.1046   Validation loss: 4.0164\n",
      "Epoch: 190   Training loss: 4.0284   Validation loss: 3.9408\n",
      "Epoch: 200   Training loss: 3.9708   Validation loss: 3.8755\n",
      "Epoch: 210   Training loss: 3.9426   Validation loss: 3.8173\n",
      "Epoch: 220   Training loss: 3.8867   Validation loss: 3.7661\n",
      "Epoch: 230   Training loss: 3.7911   Validation loss: 3.7200\n",
      "Epoch: 240   Training loss: 3.7947   Validation loss: 3.6780\n",
      "Epoch: 250   Training loss: 3.7296   Validation loss: 3.6417\n",
      "Epoch: 260   Training loss: 3.7205   Validation loss: 3.6079\n",
      "Epoch: 270   Training loss: 3.7411   Validation loss: 3.5768\n",
      "Epoch: 280   Training loss: 3.6577   Validation loss: 3.5479\n",
      "Epoch: 290   Training loss: 3.6648   Validation loss: 3.5220\n",
      "Epoch: 300   Training loss: 3.6639   Validation loss: 3.4993\n",
      "Epoch: 310   Training loss: 3.6233   Validation loss: 3.4773\n",
      "Epoch: 320   Training loss: 3.5967   Validation loss: 3.4554\n",
      "Epoch: 330   Training loss: 3.6083   Validation loss: 3.4376\n",
      "Epoch: 340   Training loss: 3.6053   Validation loss: 3.4198\n",
      "Epoch: 350   Training loss: 3.5123   Validation loss: 3.4021\n",
      "Epoch: 360   Training loss: 3.5725   Validation loss: 3.3869\n",
      "Epoch: 370   Training loss: 3.5404   Validation loss: 3.3720\n",
      "Epoch: 380   Training loss: 3.5726   Validation loss: 3.3566\n",
      "Epoch: 390   Training loss: 3.4831   Validation loss: 3.3439\n",
      "Epoch: 400   Training loss: 3.5128   Validation loss: 3.3315\n",
      "Epoch: 410   Training loss: 3.5330   Validation loss: 3.3176\n",
      "Epoch: 420   Training loss: 3.5214   Validation loss: 3.3070\n",
      "Epoch: 430   Training loss: 3.5230   Validation loss: 3.2949\n",
      "Epoch: 440   Training loss: 3.4626   Validation loss: 3.2836\n",
      "Epoch: 450   Training loss: 3.5007   Validation loss: 3.2734\n",
      "Epoch: 460   Training loss: 3.5102   Validation loss: 3.2643\n",
      "Epoch: 470   Training loss: 3.4729   Validation loss: 3.2544\n",
      "Epoch: 480   Training loss: 3.4448   Validation loss: 3.2439\n",
      "Epoch: 490   Training loss: 3.4648   Validation loss: 3.2356\n",
      "Epoch: 500   Training loss: 3.4142   Validation loss: 3.2259\n",
      "Epoch: 510   Training loss: 3.3866   Validation loss: 3.2188\n",
      "Epoch: 520   Training loss: 3.4401   Validation loss: 3.2090\n",
      "Epoch: 530   Training loss: 3.4639   Validation loss: 3.2000\n",
      "Epoch: 540   Training loss: 3.3944   Validation loss: 3.1912\n",
      "Epoch: 550   Training loss: 3.4291   Validation loss: 3.1836\n",
      "Epoch: 560   Training loss: 3.4214   Validation loss: 3.1749\n",
      "Epoch: 570   Training loss: 3.4017   Validation loss: 3.1673\n",
      "Epoch: 580   Training loss: 3.3981   Validation loss: 3.1584\n",
      "Epoch: 590   Training loss: 3.4167   Validation loss: 3.1515\n",
      "Epoch: 600   Training loss: 3.4487   Validation loss: 3.1437\n",
      "Epoch: 610   Training loss: 3.3765   Validation loss: 3.1354\n",
      "Epoch: 620   Training loss: 3.4173   Validation loss: 3.1267\n",
      "Epoch: 630   Training loss: 3.4334   Validation loss: 3.1199\n",
      "Epoch: 640   Training loss: 3.3494   Validation loss: 3.1123\n",
      "Epoch: 650   Training loss: 3.3554   Validation loss: 3.1059\n",
      "Epoch: 660   Training loss: 3.3386   Validation loss: 3.0988\n",
      "Epoch: 670   Training loss: 3.4193   Validation loss: 3.0913\n",
      "Epoch: 680   Training loss: 3.3840   Validation loss: 3.0837\n",
      "Epoch: 690   Training loss: 3.3391   Validation loss: 3.0771\n",
      "Epoch: 700   Training loss: 3.3325   Validation loss: 3.0724\n",
      "Epoch: 710   Training loss: 3.3425   Validation loss: 3.0650\n",
      "Epoch: 720   Training loss: 3.3602   Validation loss: 3.0571\n",
      "Epoch: 730   Training loss: 3.3452   Validation loss: 3.0499\n",
      "Epoch: 740   Training loss: 3.3357   Validation loss: 3.0442\n",
      "Epoch: 750   Training loss: 3.3473   Validation loss: 3.0398\n",
      "Epoch: 760   Training loss: 3.3090   Validation loss: 3.0329\n",
      "Epoch: 770   Training loss: 3.3055   Validation loss: 3.0283\n",
      "Epoch: 780   Training loss: 3.3788   Validation loss: 3.0230\n",
      "Epoch: 790   Training loss: 3.3187   Validation loss: 3.0182\n",
      "Epoch: 800   Training loss: 3.2749   Validation loss: 3.0104\n",
      "Epoch: 810   Training loss: 3.3053   Validation loss: 3.0056\n",
      "Epoch: 820   Training loss: 3.3385   Validation loss: 2.9998\n",
      "Epoch: 830   Training loss: 3.3036   Validation loss: 2.9948\n",
      "Epoch: 840   Training loss: 3.2833   Validation loss: 2.9892\n",
      "Epoch: 850   Training loss: 3.2941   Validation loss: 2.9847\n",
      "Epoch: 860   Training loss: 3.2749   Validation loss: 2.9788\n",
      "Epoch: 870   Training loss: 3.2805   Validation loss: 2.9737\n",
      "Epoch: 880   Training loss: 3.2954   Validation loss: 2.9697\n",
      "Epoch: 890   Training loss: 3.3350   Validation loss: 2.9654\n",
      "Epoch: 900   Training loss: 3.2788   Validation loss: 2.9610\n",
      "Epoch: 910   Training loss: 3.2570   Validation loss: 2.9568\n",
      "Epoch: 920   Training loss: 3.2766   Validation loss: 2.9530\n",
      "Epoch: 930   Training loss: 3.3061   Validation loss: 2.9488\n",
      "Epoch: 940   Training loss: 3.3121   Validation loss: 2.9454\n",
      "Epoch: 950   Training loss: 3.2778   Validation loss: 2.9400\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "train_validate_rt_model(model, train_dataloader, val_dataloader, loss_function, optimizer, num_epochs, patience, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "test_predictions = model(x_test).squeeze(-1)\n",
    "\n",
    "survival_predictions = test_predictions[:, :, 0:1].squeeze(-1)\n",
    "recurrent_predictions = test_predictions[:, :, 1:2].squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'25th Quantile CI': 0.9565789473684212,\n",
       " '50th Quantile CI': 0.9662366156188291,\n",
       " '75th Quantile CI': 0.9662366156188291}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_survival_metrics(survival_predictions, d_train, t_train, d_test, t_test, d_val, t_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'25th Quantile CI': 0.7277529459569281,\n",
       " '50th Quantile CI': 0.5647342995169082,\n",
       " '75th Quantile CI': 0.5647342995169082}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recurrent_cindex(recurrent_predictions, e_test, t_test, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0   Training loss: 5.2783   Validation loss: 4.9569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0   Training loss: 5.1055   Validation loss: 5.1536\n",
      "Early stopping\n",
      "Epoch: 0   Training loss: 5.6743   Validation loss: 5.6215\n",
      "Epoch: 1000   Training loss: 3.2161   Validation loss: 2.8989\n",
      "Early stopping\n",
      "Epoch: 0   Training loss: 4.9109   Validation loss: 4.7191\n",
      "Early stopping\n",
      "Epoch: 0   Training loss: 5.8840   Validation loss: 5.8265\n",
      "Early stopping\n",
      "Epoch: 0   Training loss: 5.1185   Validation loss: 4.9886\n",
      "Early stopping\n",
      "Epoch: 0   Training loss: 4.9664   Validation loss: 4.7394\n",
      "Early stopping\n",
      "Epoch: 0   Training loss: 4.7094   Validation loss: 4.5858\n",
      "Epoch: 1000   Training loss: 3.1888   Validation loss: 2.8866\n",
      "Early stopping\n",
      "Epoch: 0   Training loss: 4.8085   Validation loss: 4.5682\n",
      "Early stopping\n",
      "Epoch: 0   Training loss: 6.0655   Validation loss: 5.8072\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "nsamples = 10\n",
    "survival_cis = []\n",
    "recurrent_cis = []\n",
    "for sample in range(0, nsamples): \n",
    "    print(\"sample: \",sample)\n",
    "    readmission_tensor = _prepare_rt_tensors(x, t, e, d)\n",
    "    locals().update(readmission_tensor) # create variables from dictionary\n",
    "\n",
    "    train_dataloader, val_dataloader, test_dataloader = _prepare_dataloaders(readmission_tensor, batch_size=32)\n",
    "\n",
    "    input_size = len(x[0]) \n",
    "    output_size = int(max(t))\n",
    "    hidden_size = 8   # Number of units in the RNN layer\n",
    "\n",
    "    model = SimpleLSTM(input_size, hidden_size, output_size, 2, 0.1)\n",
    "\n",
    "    # Instantiate the model\n",
    "\n",
    "    num_epochs = 10000\n",
    "    patience = 3  # Number of epochs to wait for improvement before stopping\n",
    "    best_val_loss = float('inf')\n",
    "    wait = 0\n",
    "    loss_function = recurrent_terminal_loss\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),  lr=1e-4, weight_decay=1e-7)\n",
    "    train_validate_rt_model(model, train_dataloader, val_dataloader, loss_function, optimizer, num_epochs, patience, print_every=1000)\n",
    "    test_predictions = model(x_test).squeeze(-1)\n",
    "\n",
    "    survival_predictions = test_predictions[:, :, 0:1].squeeze(-1)\n",
    "    recurrent_predictions = test_predictions[:, :, 1:2].squeeze(-1)\n",
    "    survival_cis.append(calculate_survival_metrics(survival_predictions, d_train, t_train, d_test, t_test, d_val, t_val))\n",
    "    recurrent_cis.append(recurrent_cindex(recurrent_predictions, e_test, t_test, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation for the c-index of 25th quantile: 0.01306281436420088\n",
      "Standard deviation for the c-index of 50th quantile: 0.01483553475574478\n",
      "Standard deviation for the c-index of 75th quantile: 0.022048538422807778\n"
     ]
    }
   ],
   "source": [
    "survival_cis_25th = [survival_ci['25th Quantile CI'] for  survival_ci in survival_cis]\n",
    "print(\"Standard deviation for the c-index of 25th quantile:\", np.std(np.array(survival_cis_25th)))\n",
    "\n",
    "survival_cis_50th = [survival_ci['50th Quantile CI'] for survival_ci in survival_cis]\n",
    "print(\"Standard deviation for the c-index of 50th quantile:\", np.std(np.array(survival_cis_50th)))\n",
    "\n",
    "survival_cis_75th = [survival_ci['75th Quantile CI'] for survival_ci in survival_cis]\n",
    "print(\"Standard deviation for the c-index of 75th quantile:\", np.std(np.array(survival_cis_75th)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
