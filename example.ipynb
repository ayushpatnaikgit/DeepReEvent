{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from src.data import _load_readmission_dataset\n",
    "from src.helper_functions import _prepare_rt_tensors\n",
    "from src.models import *\n",
    "from src.losses import *\n",
    "from src.training import *\n",
    "from src.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayush/DeepReEvent/src/data.py:45: FutureWarning: The provided callable <built-in function max> is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  data['max_time'] = data.groupby('id')['t.stop'].transform(max)\n"
     ]
    }
   ],
   "source": [
    "x, t, e, d = _load_readmission_dataset(sequential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.tensor(t) == 0).sum() ## another problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayush/DeepReEvent/src/helper_functions.py:83: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "readmission_tensor = _prepare_rt_tensors(x, t, e, d)\n",
    "locals().update(readmission_tensor) # create variables from dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# model and training parameters\n",
    "\n",
    "input_size = len(x[0]) \n",
    "output_size = int(max(t))\n",
    "hidden_size = 8   # Number of units in the RNN layer\n",
    "\n",
    "model = SimpleRNN(input_size, hidden_size, output_size, 2)\n",
    "\n",
    "# Instantiate the model\n",
    "\n",
    "num_epochs = 3000\n",
    "patience = 3  # Number of epochs to wait for improvement before stopping\n",
    "best_val_loss = float('inf')\n",
    "wait = 0\n",
    "loss_function = recurrent_terminal_loss\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),  lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#prepare data loaders\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train, t_train, e_train, d_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(x_val, t_val, e_val, d_val)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0   Training loss: 4.9917   Validation loss: 4.7852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10   Training loss: 4.8317   Validation loss: 4.6499\n",
      "Epoch: 20   Training loss: 4.6845   Validation loss: 4.5214\n",
      "Epoch: 30   Training loss: 4.5485   Validation loss: 4.3997\n",
      "Epoch: 40   Training loss: 4.4232   Validation loss: 4.2852\n",
      "Epoch: 50   Training loss: 4.3091   Validation loss: 4.1791\n",
      "Epoch: 60   Training loss: 4.2069   Validation loss: 4.0828\n",
      "Epoch: 70   Training loss: 4.1170   Validation loss: 3.9968\n",
      "Epoch: 80   Training loss: 4.0389   Validation loss: 3.9211\n",
      "Epoch: 90   Training loss: 3.9713   Validation loss: 3.8546\n",
      "Epoch: 100   Training loss: 3.9127   Validation loss: 3.7963\n",
      "Epoch: 110   Training loss: 3.8618   Validation loss: 3.7451\n",
      "Epoch: 120   Training loss: 3.8173   Validation loss: 3.6998\n",
      "Epoch: 130   Training loss: 3.7781   Validation loss: 3.6594\n",
      "Epoch: 140   Training loss: 3.7434   Validation loss: 3.6232\n",
      "Epoch: 150   Training loss: 3.7125   Validation loss: 3.5906\n",
      "Epoch: 160   Training loss: 3.6847   Validation loss: 3.5610\n",
      "Epoch: 170   Training loss: 3.6597   Validation loss: 3.5340\n",
      "Epoch: 180   Training loss: 3.6370   Validation loss: 3.5092\n",
      "Epoch: 190   Training loss: 3.6163   Validation loss: 3.4863\n",
      "Epoch: 200   Training loss: 3.5974   Validation loss: 3.4652\n",
      "Epoch: 210   Training loss: 3.5801   Validation loss: 3.4455\n",
      "Epoch: 220   Training loss: 3.5641   Validation loss: 3.4271\n",
      "Epoch: 230   Training loss: 3.5494   Validation loss: 3.4100\n",
      "Epoch: 240   Training loss: 3.5357   Validation loss: 3.3938\n",
      "Epoch: 250   Training loss: 3.5230   Validation loss: 3.3787\n",
      "Epoch: 260   Training loss: 3.5111   Validation loss: 3.3644\n",
      "Epoch: 270   Training loss: 3.5000   Validation loss: 3.3509\n",
      "Epoch: 280   Training loss: 3.4896   Validation loss: 3.3381\n",
      "Epoch: 290   Training loss: 3.4798   Validation loss: 3.3260\n",
      "Epoch: 300   Training loss: 3.4706   Validation loss: 3.3145\n",
      "Epoch: 310   Training loss: 3.4618   Validation loss: 3.3036\n",
      "Epoch: 320   Training loss: 3.4534   Validation loss: 3.2932\n",
      "Epoch: 330   Training loss: 3.4454   Validation loss: 3.2833\n",
      "Epoch: 340   Training loss: 3.4378   Validation loss: 3.2738\n",
      "Epoch: 350   Training loss: 3.4304   Validation loss: 3.2647\n",
      "Epoch: 360   Training loss: 3.4233   Validation loss: 3.2561\n",
      "Epoch: 370   Training loss: 3.4165   Validation loss: 3.2478\n",
      "Epoch: 380   Training loss: 3.4098   Validation loss: 3.2399\n",
      "Epoch: 390   Training loss: 3.4033   Validation loss: 3.2323\n",
      "Epoch: 400   Training loss: 3.3970   Validation loss: 3.2250\n",
      "Epoch: 410   Training loss: 3.3909   Validation loss: 3.2180\n",
      "Epoch: 420   Training loss: 3.3849   Validation loss: 3.2113\n",
      "Epoch: 430   Training loss: 3.3790   Validation loss: 3.2049\n",
      "Epoch: 440   Training loss: 3.3732   Validation loss: 3.1987\n",
      "Epoch: 450   Training loss: 3.3676   Validation loss: 3.1927\n",
      "Epoch: 460   Training loss: 3.3621   Validation loss: 3.1870\n",
      "Epoch: 470   Training loss: 3.3567   Validation loss: 3.1815\n",
      "Epoch: 480   Training loss: 3.3514   Validation loss: 3.1763\n",
      "Epoch: 490   Training loss: 3.3462   Validation loss: 3.1712\n",
      "Epoch: 500   Training loss: 3.3411   Validation loss: 3.1664\n",
      "Epoch: 510   Training loss: 3.3362   Validation loss: 3.1618\n",
      "Epoch: 520   Training loss: 3.3313   Validation loss: 3.1573\n",
      "Epoch: 530   Training loss: 3.3266   Validation loss: 3.1531\n",
      "Epoch: 540   Training loss: 3.3219   Validation loss: 3.1491\n",
      "Epoch: 550   Training loss: 3.3174   Validation loss: 3.1453\n",
      "Epoch: 560   Training loss: 3.3129   Validation loss: 3.1417\n",
      "Epoch: 570   Training loss: 3.3086   Validation loss: 3.1383\n",
      "Epoch: 580   Training loss: 3.3043   Validation loss: 3.1351\n",
      "Epoch: 590   Training loss: 3.3002   Validation loss: 3.1321\n",
      "Epoch: 600   Training loss: 3.2961   Validation loss: 3.1293\n",
      "Epoch: 610   Training loss: 3.2921   Validation loss: 3.1267\n",
      "Epoch: 620   Training loss: 3.2882   Validation loss: 3.1242\n",
      "Epoch: 630   Training loss: 3.2843   Validation loss: 3.1220\n",
      "Epoch: 640   Training loss: 3.2805   Validation loss: 3.1199\n",
      "Epoch: 650   Training loss: 3.2767   Validation loss: 3.1180\n",
      "Epoch: 660   Training loss: 3.2730   Validation loss: 3.1162\n",
      "Epoch: 670   Training loss: 3.2693   Validation loss: 3.1146\n",
      "Epoch: 680   Training loss: 3.2657   Validation loss: 3.1131\n",
      "Epoch: 690   Training loss: 3.2621   Validation loss: 3.1117\n",
      "Epoch: 700   Training loss: 3.2585   Validation loss: 3.1105\n",
      "Epoch: 710   Training loss: 3.2550   Validation loss: 3.1093\n",
      "Epoch: 720   Training loss: 3.2515   Validation loss: 3.1083\n",
      "Epoch: 730   Training loss: 3.2479   Validation loss: 3.1073\n",
      "Epoch: 740   Training loss: 3.2444   Validation loss: 3.1065\n",
      "Epoch: 750   Training loss: 3.2409   Validation loss: 3.1057\n",
      "Epoch: 760   Training loss: 3.2374   Validation loss: 3.1050\n",
      "Epoch: 770   Training loss: 3.2340   Validation loss: 3.1043\n",
      "Epoch: 780   Training loss: 3.2305   Validation loss: 3.1037\n",
      "Epoch: 790   Training loss: 3.2270   Validation loss: 3.1031\n",
      "Epoch: 800   Training loss: 3.2235   Validation loss: 3.1025\n",
      "Epoch: 810   Training loss: 3.2199   Validation loss: 3.1019\n",
      "Epoch: 820   Training loss: 3.2164   Validation loss: 3.1014\n",
      "Epoch: 830   Training loss: 3.2128   Validation loss: 3.1008\n",
      "Epoch: 840   Training loss: 3.2093   Validation loss: 3.1002\n",
      "Epoch: 850   Training loss: 3.2057   Validation loss: 3.0995\n",
      "Epoch: 860   Training loss: 3.2020   Validation loss: 3.0988\n",
      "Epoch: 870   Training loss: 3.1983   Validation loss: 3.0981\n",
      "Epoch: 880   Training loss: 3.1946   Validation loss: 3.0972\n",
      "Epoch: 890   Training loss: 3.1909   Validation loss: 3.0963\n",
      "Epoch: 900   Training loss: 3.1871   Validation loss: 3.0953\n",
      "Epoch: 910   Training loss: 3.1833   Validation loss: 3.0941\n",
      "Epoch: 920   Training loss: 3.1794   Validation loss: 3.0929\n",
      "Epoch: 930   Training loss: 3.1755   Validation loss: 3.0915\n",
      "Epoch: 940   Training loss: 3.1716   Validation loss: 3.0900\n",
      "Epoch: 950   Training loss: 3.1676   Validation loss: 3.0884\n",
      "Epoch: 960   Training loss: 3.1636   Validation loss: 3.0867\n",
      "Epoch: 970   Training loss: 3.1595   Validation loss: 3.0848\n",
      "Epoch: 980   Training loss: 3.1554   Validation loss: 3.0829\n",
      "Epoch: 990   Training loss: 3.1513   Validation loss: 3.0808\n",
      "Epoch: 1000   Training loss: 3.1472   Validation loss: 3.0787\n",
      "Epoch: 1010   Training loss: 3.1430   Validation loss: 3.0764\n",
      "Epoch: 1020   Training loss: 3.1388   Validation loss: 3.0741\n",
      "Epoch: 1030   Training loss: 3.1346   Validation loss: 3.0718\n",
      "Epoch: 1040   Training loss: 3.1304   Validation loss: 3.0693\n",
      "Epoch: 1050   Training loss: 3.1262   Validation loss: 3.0669\n",
      "Epoch: 1060   Training loss: 3.1220   Validation loss: 3.0644\n",
      "Epoch: 1070   Training loss: 3.1179   Validation loss: 3.0619\n",
      "Epoch: 1080   Training loss: 3.1137   Validation loss: 3.0593\n",
      "Epoch: 1090   Training loss: 3.1095   Validation loss: 3.0568\n",
      "Epoch: 1100   Training loss: 3.1054   Validation loss: 3.0543\n",
      "Epoch: 1110   Training loss: 3.1014   Validation loss: 3.0519\n",
      "Epoch: 1120   Training loss: 3.0974   Validation loss: 3.0494\n",
      "Epoch: 1130   Training loss: 3.0934   Validation loss: 3.0470\n",
      "Epoch: 1140   Training loss: 3.0895   Validation loss: 3.0447\n",
      "Epoch: 1150   Training loss: 3.0857   Validation loss: 3.0424\n",
      "Epoch: 1160   Training loss: 3.0820   Validation loss: 3.0402\n",
      "Epoch: 1170   Training loss: 3.0784   Validation loss: 3.0381\n",
      "Epoch: 1180   Training loss: 3.0749   Validation loss: 3.0361\n",
      "Epoch: 1190   Training loss: 3.0714   Validation loss: 3.0342\n",
      "Epoch: 1200   Training loss: 3.0681   Validation loss: 3.0323\n",
      "Epoch: 1210   Training loss: 3.0649   Validation loss: 3.0306\n",
      "Epoch: 1220   Training loss: 3.0618   Validation loss: 3.0290\n",
      "Epoch: 1230   Training loss: 3.0588   Validation loss: 3.0274\n",
      "Epoch: 1240   Training loss: 3.0559   Validation loss: 3.0260\n",
      "Epoch: 1250   Training loss: 3.0531   Validation loss: 3.0247\n",
      "Epoch: 1260   Training loss: 3.0504   Validation loss: 3.0235\n",
      "Epoch: 1270   Training loss: 3.0478   Validation loss: 3.0224\n",
      "Epoch: 1280   Training loss: 3.0454   Validation loss: 3.0215\n",
      "Epoch: 1290   Training loss: 3.0430   Validation loss: 3.0206\n",
      "Epoch: 1300   Training loss: 3.0407   Validation loss: 3.0198\n",
      "Epoch: 1310   Training loss: 3.0385   Validation loss: 3.0192\n",
      "Epoch: 1320   Training loss: 3.0363   Validation loss: 3.0186\n",
      "Epoch: 1330   Training loss: 3.0343   Validation loss: 3.0181\n",
      "Epoch: 1340   Training loss: 3.0323   Validation loss: 3.0178\n",
      "Epoch: 1350   Training loss: 3.0305   Validation loss: 3.0175\n",
      "Epoch: 1360   Training loss: 3.0287   Validation loss: 3.0173\n",
      "Epoch: 1370   Training loss: 3.0269   Validation loss: 3.0172\n",
      "Epoch: 1380   Training loss: 3.0253   Validation loss: 3.0171\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "train_validate_rt_model(model, train_dataloader, val_dataloader, loss_function, optimizer, num_epochs, patience, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "test_predictions = model(x_test).squeeze(-1)\n",
    "\n",
    "survival_predictions = test_predictions[:, :, 0:1].squeeze(-1)\n",
    "recurrent_predictions = test_predictions[:, :, 1:2].squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# calculate_survival_metrics(survival_predictions, d_train, t_train, d_test, t_test, d_val, t_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(253)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(d_train == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5164, 0.1291, 0.0471, 0.0295, 0.0217, 0.0184],\n",
       "        [0.0741, 0.0287, 0.0213, 0.0194, 0.0167, 0.0146],\n",
       "        [0.1041, 0.0386, 0.0267, 0.0219, 0.0178, 0.0155],\n",
       "        ...,\n",
       "        [0.0311, 0.0124, 0.0101, 0.0094, 0.0084, 0.0079],\n",
       "        [0.1123, 0.0360, 0.0260, 0.0203, 0.0133, 0.0095],\n",
       "        [0.0487, 0.0153, 0.0106, 0.0099, 0.0094, 0.0093]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_train).squeeze(-1)[:, :, 0:1].squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.nonzero(d_train == 1, as_tuple=True)[0]\n",
    "\n",
    "# Get the first occurrence\n",
    "dead_index = indices[0].item()\n",
    "dead_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1123, 0.0360, 0.0260, 0.0203, 0.0133, 0.0095],\n",
      "       grad_fn=<SelectBackward0>) tensor(5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0133, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictions = model(x_train).squeeze(-1)[:, :, 0:1].squeeze(-1)\n",
    "h = train_predictions[dead_index]\n",
    "t = t_train[dead_index]\n",
    "print(h, t)\n",
    "\n",
    "h[t-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.2433, 0.7087, 1.3932, 2.5085, 1.4916, 2.6302, 1.4803, 2.0606, 0.6579,\n",
      "        0.9303, 0.8624, 2.4814, 2.2433, 1.4750, 2.8014, 0.5470, 0.4347, 2.0606,\n",
      "        1.8625, 2.6302, 1.3541, 1.4002, 1.8625, 1.3940, 2.6306, 2.0962, 2.9550,\n",
      "        1.6421, 1.2247, 2.0962, 1.4750, 0.8424, 0.8012, 1.4637, 0.6287, 2.0962,\n",
      "        1.4803, 0.4626, 1.6421, 1.8625, 0.9303, 1.2525, 0.7703, 0.8021, 1.2525,\n",
      "        1.8700, 0.4592, 1.4750, 0.6294, 1.5826, 0.8624, 2.0967, 0.8624, 0.9757,\n",
      "        1.3598, 1.4637, 0.8624, 1.5826, 1.2525, 1.3541, 1.2525, 0.7877, 0.7945,\n",
      "        0.9303, 1.8700, 1.4637, 0.7765, 2.2433, 1.8625, 3.0073, 1.3541, 0.4511,\n",
      "        1.4257, 0.8659, 1.4750, 0.4511, 1.9632, 1.2525, 1.5826, 0.9256, 0.7765],\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "out_risk = recurrent_predictions\n",
    "\n",
    "current_time = 20\n",
    "\n",
    "expected_number_of_events = torch.zeros(out_risk.size(0))\n",
    "\n",
    "for i in range(out_risk.size(0)):\n",
    "    clamped_time = min(current_time, t_test[i].item())\n",
    "    expected_number_of_events[i] = out_risk[i, 0:clamped_time].sum()\n",
    "\n",
    "print(expected_number_of_events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n",
      "tensor(1.4916, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = e_test < current_time\n",
    "observed_number_of_events = mask.sum(dim=1)\n",
    "\n",
    "print(observed_number_of_events[4])\n",
    "print(expected_number_of_events[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True, False])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.9443, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = out_risk[2]\n",
    "e = e_test[2]\n",
    "t = t_test[2]\n",
    "\n",
    "mask = torch.ones_like(h[0:t], dtype=torch.bool)\n",
    "# Set mask to False for time points where events occurred\n",
    "mask[e[e < t]] = False\n",
    "print(mask)\n",
    "# Calculate the negative log-likelihood for both the event occurrences and non-occurrences\n",
    "-1 * (torch.sum(torch.log(h[0:t][e[e < t]])) + torch.sum(torch.log(1 - h[0:t][mask])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h tensor([0.7703, 0.6229, 0.5700, 0.6658, 0.7384, 0.7658],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "e, t tensor([  1,   2,   2,   2, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100]) tensor(2)\n",
      "tensor([0.6229], grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"h\", h)\n",
    "print(\"e, t\", e, t)\n",
    "print(h[0:t][e[e < t]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data/readmission.csv', delimiter=';')\n",
    "torch.tensor(0.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
