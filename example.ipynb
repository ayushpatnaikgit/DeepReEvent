{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from src.data import _load_readmission_dataset, _load_simData\n",
    "from src.helper_functions import _prepare_rt_tensors\n",
    "from src.models import *\n",
    "from src.losses import *\n",
    "from src.training import *\n",
    "from src.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayush/DeepReEvent/src/data.py:47: FutureWarning: The provided callable <built-in function max> is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  data['max_time'] = data.groupby('id')['t.stop'].transform(max)\n"
     ]
    }
   ],
   "source": [
    "x, t, e, d = _load_readmission_dataset(sequential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, t, e, d = _load_simData(sequential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayush/DeepReEvent/src/helper_functions.py:83: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "readmission_tensor = _prepare_rt_tensors(x, t, e, d)\n",
    "locals().update(readmission_tensor) # create variables from dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# model and training parameters\n",
    "\n",
    "input_size = len(x[0]) \n",
    "output_size = int(max(t))\n",
    "hidden_size = 8   # Number of units in the RNN layer\n",
    "\n",
    "model = SimpleGRU(input_size, hidden_size, output_size, 2)\n",
    "\n",
    "# Instantiate the model\n",
    "\n",
    "num_epochs = 10000\n",
    "patience = 3  # Number of epochs to wait for improvement before stopping\n",
    "best_val_loss = float('inf')\n",
    "wait = 0\n",
    "loss_function = recurrent_terminal_loss\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),  lr=1e-4, weight_decay=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#prepare data loaders\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train, t_train, e_train, d_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(x_val, t_val, e_val, d_val)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0   Training loss: 15.1134   Validation loss: 13.9719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10   Training loss: 14.4475   Validation loss: 13.4327\n",
      "Epoch: 20   Training loss: 13.9421   Validation loss: 12.9642\n",
      "Epoch: 30   Training loss: 13.3933   Validation loss: 12.5372\n",
      "Epoch: 40   Training loss: 12.9289   Validation loss: 12.1228\n",
      "Epoch: 50   Training loss: 12.4089   Validation loss: 11.7020\n",
      "Epoch: 60   Training loss: 11.9593   Validation loss: 11.2677\n",
      "Epoch: 70   Training loss: 11.4280   Validation loss: 10.8268\n",
      "Epoch: 80   Training loss: 10.9830   Validation loss: 10.3950\n",
      "Epoch: 90   Training loss: 10.4318   Validation loss: 9.9897\n",
      "Epoch: 100   Training loss: 10.0720   Validation loss: 9.6172\n",
      "Epoch: 110   Training loss: 9.7550   Validation loss: 9.2856\n",
      "Epoch: 120   Training loss: 9.4719   Validation loss: 8.9915\n",
      "Epoch: 130   Training loss: 9.2031   Validation loss: 8.7314\n",
      "Epoch: 140   Training loss: 8.9787   Validation loss: 8.4996\n",
      "Epoch: 150   Training loss: 8.6064   Validation loss: 8.2930\n",
      "Epoch: 160   Training loss: 8.6190   Validation loss: 8.1096\n",
      "Epoch: 170   Training loss: 8.3343   Validation loss: 7.9450\n",
      "Epoch: 180   Training loss: 8.1786   Validation loss: 7.7990\n",
      "Epoch: 190   Training loss: 8.1173   Validation loss: 7.6689\n",
      "Epoch: 200   Training loss: 7.9884   Validation loss: 7.5524\n",
      "Epoch: 210   Training loss: 7.9329   Validation loss: 7.4493\n",
      "Epoch: 220   Training loss: 7.7636   Validation loss: 7.3571\n",
      "Epoch: 230   Training loss: 7.6351   Validation loss: 7.2733\n",
      "Epoch: 240   Training loss: 7.6877   Validation loss: 7.1981\n",
      "Epoch: 250   Training loss: 7.6657   Validation loss: 7.1306\n",
      "Epoch: 260   Training loss: 7.5957   Validation loss: 7.0684\n",
      "Epoch: 270   Training loss: 7.5906   Validation loss: 7.0112\n",
      "Epoch: 280   Training loss: 7.5691   Validation loss: 6.9580\n",
      "Epoch: 290   Training loss: 7.4964   Validation loss: 6.9092\n",
      "Epoch: 300   Training loss: 7.4417   Validation loss: 6.8636\n",
      "Epoch: 310   Training loss: 7.2830   Validation loss: 6.8214\n",
      "Epoch: 320   Training loss: 7.4384   Validation loss: 6.7819\n",
      "Epoch: 330   Training loss: 7.3707   Validation loss: 6.7453\n",
      "Epoch: 340   Training loss: 7.2782   Validation loss: 6.7106\n",
      "Epoch: 350   Training loss: 7.3412   Validation loss: 6.6776\n",
      "Epoch: 360   Training loss: 7.3095   Validation loss: 6.6470\n",
      "Epoch: 370   Training loss: 7.3580   Validation loss: 6.6174\n",
      "Epoch: 380   Training loss: 7.2829   Validation loss: 6.5877\n",
      "Epoch: 390   Training loss: 7.2704   Validation loss: 6.5612\n",
      "Epoch: 400   Training loss: 7.2700   Validation loss: 6.5345\n",
      "Epoch: 410   Training loss: 7.2095   Validation loss: 6.5092\n",
      "Epoch: 420   Training loss: 7.0565   Validation loss: 6.4860\n",
      "Epoch: 430   Training loss: 7.0414   Validation loss: 6.4619\n",
      "Epoch: 440   Training loss: 6.9903   Validation loss: 6.4391\n",
      "Epoch: 450   Training loss: 7.1351   Validation loss: 6.4176\n",
      "Epoch: 460   Training loss: 6.9996   Validation loss: 6.3956\n",
      "Epoch: 470   Training loss: 7.0497   Validation loss: 6.3745\n",
      "Epoch: 480   Training loss: 7.0594   Validation loss: 6.3551\n",
      "Epoch: 490   Training loss: 6.9898   Validation loss: 6.3344\n",
      "Epoch: 500   Training loss: 7.0285   Validation loss: 6.3143\n",
      "Epoch: 510   Training loss: 6.9699   Validation loss: 6.2948\n",
      "Epoch: 520   Training loss: 7.0229   Validation loss: 6.2746\n",
      "Epoch: 530   Training loss: 6.9059   Validation loss: 6.2551\n",
      "Epoch: 540   Training loss: 6.9957   Validation loss: 6.2373\n",
      "Epoch: 550   Training loss: 6.9965   Validation loss: 6.2196\n",
      "Epoch: 560   Training loss: 6.9143   Validation loss: 6.2022\n",
      "Epoch: 570   Training loss: 6.8880   Validation loss: 6.1846\n",
      "Epoch: 580   Training loss: 7.0246   Validation loss: 6.1675\n",
      "Epoch: 590   Training loss: 6.9786   Validation loss: 6.1498\n",
      "Epoch: 600   Training loss: 7.0712   Validation loss: 6.1335\n",
      "Epoch: 610   Training loss: 7.0307   Validation loss: 6.1165\n",
      "Epoch: 620   Training loss: 7.0498   Validation loss: 6.1032\n",
      "Epoch: 630   Training loss: 6.9978   Validation loss: 6.0871\n",
      "Epoch: 640   Training loss: 6.9201   Validation loss: 6.0706\n",
      "Epoch: 650   Training loss: 6.9609   Validation loss: 6.0565\n",
      "Epoch: 660   Training loss: 6.8544   Validation loss: 6.0412\n",
      "Epoch: 670   Training loss: 6.9782   Validation loss: 6.0300\n",
      "Epoch: 680   Training loss: 6.8032   Validation loss: 6.0155\n",
      "Epoch: 690   Training loss: 6.9398   Validation loss: 6.0040\n",
      "Epoch: 700   Training loss: 6.9169   Validation loss: 5.9919\n",
      "Epoch: 710   Training loss: 6.8297   Validation loss: 5.9812\n",
      "Epoch: 720   Training loss: 6.7204   Validation loss: 5.9714\n",
      "Epoch: 730   Training loss: 6.9052   Validation loss: 5.9618\n",
      "Epoch: 740   Training loss: 6.8599   Validation loss: 5.9514\n",
      "Epoch: 750   Training loss: 6.8298   Validation loss: 5.9427\n",
      "Epoch: 760   Training loss: 6.8338   Validation loss: 5.9332\n",
      "Epoch: 770   Training loss: 6.8042   Validation loss: 5.9254\n",
      "Epoch: 780   Training loss: 6.8392   Validation loss: 5.9182\n",
      "Epoch: 790   Training loss: 6.9090   Validation loss: 5.9103\n",
      "Epoch: 800   Training loss: 6.7184   Validation loss: 5.9014\n",
      "Epoch: 810   Training loss: 6.7893   Validation loss: 5.8941\n",
      "Epoch: 820   Training loss: 6.8287   Validation loss: 5.8884\n",
      "Epoch: 830   Training loss: 6.8249   Validation loss: 5.8819\n",
      "Epoch: 840   Training loss: 6.7977   Validation loss: 5.8764\n",
      "Epoch: 850   Training loss: 6.6893   Validation loss: 5.8726\n",
      "Epoch: 860   Training loss: 6.7616   Validation loss: 5.8675\n",
      "Epoch: 870   Training loss: 6.7235   Validation loss: 5.8649\n",
      "Epoch: 880   Training loss: 6.7830   Validation loss: 5.8608\n",
      "Epoch: 890   Training loss: 6.7599   Validation loss: 5.8566\n",
      "Epoch: 900   Training loss: 6.8196   Validation loss: 5.8541\n",
      "Epoch: 910   Training loss: 6.8109   Validation loss: 5.8511\n",
      "Epoch: 920   Training loss: 6.6587   Validation loss: 5.8480\n",
      "Epoch: 930   Training loss: 6.8384   Validation loss: 5.8448\n",
      "Epoch: 940   Training loss: 6.7512   Validation loss: 5.8419\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "train_validate_rt_model(model, train_dataloader, val_dataloader, loss_function, optimizer, num_epochs, patience, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "test_predictions = model(x_test).squeeze(-1)\n",
    "\n",
    "survival_predictions = test_predictions[:, :, 0:1].squeeze(-1)\n",
    "recurrent_predictions = test_predictions[:, :, 1:2].squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Brier Score': array([0.01919344, 0.02401563, 0.04047371, 0.03178264, 0.03209171,\n",
       "        0.02752203, 0.02841207, 0.02636668, 0.02722039, 0.02905341,\n",
       "        0.02462574, 0.0253025 , 0.01848155, 0.01853445, 0.01476412,\n",
       "        0.00860042]),\n",
       " '25th Quantile CI': 0.9598488074374354,\n",
       " '50th Quantile CI': 0.957478908064154,\n",
       " '75th Quantile CI': 0.9514659842100678}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_survival_metrics(survival_predictions, d_train, t_train, d_test, t_test, d_val, t_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.48411552346570397,\n",
       " 0.5084164588528678,\n",
       " 0.5002613695765813,\n",
       " 0.4859154929577465,\n",
       " 0.5352668213457077,\n",
       " 0.5566622399291722,\n",
       " 0.5678713089466726,\n",
       " 0.5915102389078498,\n",
       " 0.6328157784305497,\n",
       " 0.6627955205309001,\n",
       " 0.6602277348515657,\n",
       " 0.6526859504132232,\n",
       " 0.607158446093409,\n",
       " 0.5935469900389779,\n",
       " 0.5512007249660172,\n",
       " 0.510643330179754,\n",
       " 0.5016908212560387,\n",
       " 0.5016908212560387,\n",
       " 0.5016908212560387,\n",
       " 0.5016908212560387,\n",
       " 0.5016908212560387,\n",
       " 0.5016908212560387,\n",
       " 0.5016908212560387,\n",
       " 0.5016908212560387]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "recurrent_cindex(recurrent_predictions, e_test, t_test, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48411552346570397, 0.5084164588528678, 0.5002613695765813, 0.4859154929577465, 0.5352668213457077, 0.5566622399291722, 0.5678713089466726, 0.5915102389078498]\n"
     ]
    }
   ],
   "source": [
    "max_time = 10\n",
    "out_risk = recurrent_predictions\n",
    "event_times = e_test\n",
    "cindices = []\n",
    "for current_time in range(2, max_time, 1):\n",
    "    expected_number_of_events = torch.zeros(out_risk.size(0))\n",
    "    for i in range(out_risk.size(0)):\n",
    "        clamped_time = min(current_time, t_test[i].item())\n",
    "        expected_number_of_events[i] = out_risk[i, 0:clamped_time].sum()\n",
    "    mask = event_times < current_time\n",
    "    observed_number_of_events = mask.sum(dim=1)\n",
    "    concordant_pairs = 0\n",
    "    total_pairs = 0\n",
    "\n",
    "    n = len(expected_number_of_events)\n",
    "    concordant_pairs = 0\n",
    "    permissible_pairs = 0\n",
    "    tied_risk_pairs = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                if observed_number_of_events[i] < observed_number_of_events[j]:\n",
    "                    permissible_pairs += 1\n",
    "                    if expected_number_of_events[i] > expected_number_of_events[j]:\n",
    "                        concordant_pairs += 1\n",
    "                    elif expected_number_of_events[i] == expected_number_of_events[j]:\n",
    "                        tied_risk_pairs += 1\n",
    "                        # print(tied_risk_pairs)\n",
    "\n",
    "    cindices.append((concordant_pairs + 0.5 * tied_risk_pairs) / permissible_pairs)\n",
    "\n",
    "    # for i in range(0, len(expected_number_of_events)):\n",
    "    #     for j in range(0, len(expected_number_of_events)):\n",
    "    #         if i == j: \n",
    "    #             continue\n",
    "    #         if (expected_number_of_events[i] > expected_number_of_events[j]) and (observed_number_of_events[i] > observed_number_of_events[j]):\n",
    "    #             concordant_pairs += 1\n",
    "    #         total_pairs += 1  \n",
    "    # cindices.append(concordant_pairs / total_pairs)\n",
    "\n",
    "print(cindices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.8188, 1.1345, 2.7816, 1.7172, 2.0340, 1.9411, 1.6031, 1.6037, 0.6998,\n",
       "        1.6031, 0.4780, 1.7606, 1.8188, 1.6423, 2.0340, 1.0943, 0.6595, 1.6037,\n",
       "        1.7172, 1.9411, 1.7416, 1.6783, 1.7172, 1.9950, 1.8552, 1.6423, 1.8188,\n",
       "        2.4933, 1.0767, 1.6423, 1.6423, 1.6972, 0.4361, 1.6037, 0.4908, 1.6423,\n",
       "        1.6031, 0.2635, 2.4933, 1.7172, 1.6031, 1.7172, 1.1955, 0.9972, 1.7172,\n",
       "        1.7606, 0.2626, 1.6423, 0.8502, 1.8188, 1.8792, 1.7819, 1.4214, 1.6423,\n",
       "        1.8287, 1.6037, 0.9547, 1.8188, 1.5352, 1.7416, 1.5352, 0.4661, 1.0714,\n",
       "        1.6031, 1.7606, 1.6037, 0.8076, 1.8188, 1.7172, 2.5002, 1.7416, 0.6256,\n",
       "        1.6496, 1.6474, 1.6423, 0.6256, 3.0829, 1.7172, 1.8188, 1.6037, 1.1714],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_number_of_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(253)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(d_train == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1813, 0.0900, 0.0484,  ..., 0.0168, 0.0168, 0.0168],\n",
       "        [0.0292, 0.0119, 0.0106,  ..., 0.0105, 0.0105, 0.0105],\n",
       "        [0.0414, 0.0153, 0.0124,  ..., 0.0118, 0.0118, 0.0118],\n",
       "        ...,\n",
       "        [0.0223, 0.0109, 0.0102,  ..., 0.0101, 0.0101, 0.0101],\n",
       "        [0.0415, 0.0126, 0.0103,  ..., 0.0100, 0.0100, 0.0100],\n",
       "        [0.0357, 0.0127, 0.0108,  ..., 0.0106, 0.0106, 0.0106]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_train).squeeze(-1)[:, :, 0:1].squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 50 is out of bounds for dimension 0 with size 29",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnonzero(d_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Get the first occurrence\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m dead_index \u001b[38;5;241m=\u001b[39m indices[\u001b[38;5;241m50\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      5\u001b[0m dead_index\n",
      "\u001b[0;31mIndexError\u001b[0m: index 50 is out of bounds for dimension 0 with size 29"
     ]
    }
   ],
   "source": [
    "indices = torch.nonzero(d_train == 1, as_tuple=True)[0]\n",
    "\n",
    "# Get the first occurrence\n",
    "dead_index = indices[50].item()\n",
    "dead_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3354, 0.0651, 0.0542, 0.0346, 0.0192, 0.0094, 0.0049, 0.0042, 0.0071,\n",
      "        0.0150, 0.0225, 0.0205], grad_fn=<SelectBackward0>) tensor(1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.3354, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictions = model(x_train).squeeze(-1)[:, :, 0:1].squeeze(-1)\n",
    "h = train_predictions[dead_index]\n",
    "t = t_train[dead_index]\n",
    "print(h, t)\n",
    "\n",
    "h[t-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9104, 0.8103, 0.7153, 0.5897, 0.5036, 0.4383, 0.3696, 0.3012, 0.2769,\n",
       "        0.3584, 0.6184, 0.8763], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recurrent_predictions[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_risk = recurrent_predictions\n",
    "\n",
    "current_time = 20\n",
    "\n",
    "expected_number_of_events = torch.zeros(out_risk.size(0))\n",
    "\n",
    "for i in range(out_risk.size(0)):\n",
    "    clamped_time = min(current_time, t_test[i].item())\n",
    "    expected_number_of_events[i] = out_risk[i, 0:clamped_time].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n",
      "tensor(5.4015, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = e_test < current_time\n",
    "observed_number_of_events = mask.sum(dim=1)\n",
    "\n",
    "print(observed_number_of_events[6])\n",
    "print(expected_number_of_events[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times = np.arange(1, max(t_test).long(), 1)\n",
    "times"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
