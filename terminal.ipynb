{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equivalent to Ren Etal 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from deepreevent import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayush/DeepReEvent/deepreevent/data.py:47: FutureWarning: The provided callable <built-in function max> is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  data['max_time'] = data.groupby('id')['t.stop'].transform(max)\n"
     ]
    }
   ],
   "source": [
    "x, t, e, d = _load_readmission_dataset(sequential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "readmission_tensor = _prepare_rt_tensors(x, t, e, d)\n",
    "locals().update(readmission_tensor) # create variables from dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# model and training parameters\n",
    "\n",
    "input_size = len(x[0]) \n",
    "output_size = int(max(t))\n",
    "hidden_size = 8   # Number of units in the RNN layer\n",
    "\n",
    "model = SimpleGRU(input_size, hidden_size, output_size, 1, 0.1)\n",
    "\n",
    "# Instantiate the model\n",
    "\n",
    "num_epochs = 10000\n",
    "patience = 3  # Number of epochs to wait for improvement before stopping\n",
    "best_val_loss = float('inf')\n",
    "wait = 0\n",
    "loss_function = survival_loss\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),  lr=1e-4, weight_decay=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#prepare data loaders\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train, t_train, e_train, d_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(x_val, t_val, e_val, d_val)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0   Training loss: 3.4248   Validation loss: 3.0438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10   Training loss: 3.2613   Validation loss: 2.9075\n",
      "Epoch: 20   Training loss: 3.1152   Validation loss: 2.7739\n",
      "Epoch: 30   Training loss: 2.9501   Validation loss: 2.6415\n",
      "Epoch: 40   Training loss: 2.8180   Validation loss: 2.5106\n",
      "Epoch: 50   Training loss: 2.6530   Validation loss: 2.3793\n",
      "Epoch: 60   Training loss: 2.5068   Validation loss: 2.2487\n",
      "Epoch: 70   Training loss: 2.3635   Validation loss: 2.1191\n",
      "Epoch: 80   Training loss: 2.2181   Validation loss: 1.9917\n",
      "Epoch: 90   Training loss: 2.0835   Validation loss: 1.8687\n",
      "Epoch: 100   Training loss: 1.9493   Validation loss: 1.7500\n",
      "Epoch: 110   Training loss: 1.8335   Validation loss: 1.6365\n",
      "Epoch: 120   Training loss: 1.7110   Validation loss: 1.5290\n",
      "Epoch: 130   Training loss: 1.6239   Validation loss: 1.4283\n",
      "Epoch: 140   Training loss: 1.5221   Validation loss: 1.3344\n",
      "Epoch: 150   Training loss: 1.4488   Validation loss: 1.2480\n",
      "Epoch: 160   Training loss: 1.3450   Validation loss: 1.1689\n",
      "Epoch: 170   Training loss: 1.2894   Validation loss: 1.0964\n",
      "Epoch: 180   Training loss: 1.2310   Validation loss: 1.0306\n",
      "Epoch: 190   Training loss: 1.1485   Validation loss: 0.9700\n",
      "Epoch: 200   Training loss: 1.1332   Validation loss: 0.9154\n",
      "Epoch: 210   Training loss: 1.0401   Validation loss: 0.8653\n",
      "Epoch: 220   Training loss: 1.0487   Validation loss: 0.8198\n",
      "Epoch: 230   Training loss: 0.9993   Validation loss: 0.7786\n",
      "Epoch: 240   Training loss: 0.9642   Validation loss: 0.7410\n",
      "Epoch: 250   Training loss: 0.9154   Validation loss: 0.7062\n",
      "Epoch: 260   Training loss: 0.9208   Validation loss: 0.6746\n",
      "Epoch: 270   Training loss: 0.8642   Validation loss: 0.6454\n",
      "Epoch: 280   Training loss: 0.8336   Validation loss: 0.6182\n",
      "Epoch: 290   Training loss: 0.8360   Validation loss: 0.5933\n",
      "Epoch: 300   Training loss: 0.8066   Validation loss: 0.5705\n",
      "Epoch: 310   Training loss: 0.8314   Validation loss: 0.5494\n",
      "Epoch: 320   Training loss: 0.7697   Validation loss: 0.5297\n",
      "Epoch: 330   Training loss: 0.7481   Validation loss: 0.5115\n",
      "Epoch: 340   Training loss: 0.7607   Validation loss: 0.4942\n",
      "Epoch: 350   Training loss: 0.7198   Validation loss: 0.4787\n",
      "Epoch: 360   Training loss: 0.7491   Validation loss: 0.4640\n",
      "Epoch: 370   Training loss: 0.7014   Validation loss: 0.4502\n",
      "Epoch: 380   Training loss: 0.6855   Validation loss: 0.4375\n",
      "Epoch: 390   Training loss: 0.7009   Validation loss: 0.4254\n",
      "Epoch: 400   Training loss: 0.7045   Validation loss: 0.4143\n",
      "Epoch: 410   Training loss: 0.7111   Validation loss: 0.4039\n",
      "Epoch: 420   Training loss: 0.7157   Validation loss: 0.3941\n",
      "Epoch: 430   Training loss: 0.6562   Validation loss: 0.3849\n",
      "Epoch: 440   Training loss: 0.6822   Validation loss: 0.3762\n",
      "Epoch: 450   Training loss: 0.6216   Validation loss: 0.3681\n",
      "Epoch: 460   Training loss: 0.6845   Validation loss: 0.3603\n",
      "Epoch: 470   Training loss: 0.6590   Validation loss: 0.3532\n",
      "Epoch: 480   Training loss: 0.6037   Validation loss: 0.3462\n",
      "Epoch: 490   Training loss: 0.6524   Validation loss: 0.3397\n",
      "Epoch: 500   Training loss: 0.6244   Validation loss: 0.3335\n",
      "Epoch: 510   Training loss: 0.6312   Validation loss: 0.3276\n",
      "Epoch: 520   Training loss: 0.6428   Validation loss: 0.3221\n",
      "Epoch: 530   Training loss: 0.6471   Validation loss: 0.3167\n",
      "Epoch: 540   Training loss: 0.6215   Validation loss: 0.3117\n",
      "Epoch: 550   Training loss: 0.6352   Validation loss: 0.3068\n",
      "Epoch: 560   Training loss: 0.6497   Validation loss: 0.3020\n",
      "Epoch: 570   Training loss: 0.6319   Validation loss: 0.2978\n",
      "Epoch: 580   Training loss: 0.6062   Validation loss: 0.2938\n",
      "Epoch: 590   Training loss: 0.6077   Validation loss: 0.2900\n",
      "Epoch: 600   Training loss: 0.6076   Validation loss: 0.2861\n",
      "Epoch: 610   Training loss: 0.6420   Validation loss: 0.2825\n",
      "Epoch: 620   Training loss: 0.6534   Validation loss: 0.2791\n",
      "Epoch: 630   Training loss: 0.6241   Validation loss: 0.2760\n",
      "Epoch: 640   Training loss: 0.5563   Validation loss: 0.2729\n",
      "Epoch: 650   Training loss: 0.6333   Validation loss: 0.2699\n",
      "Epoch: 660   Training loss: 0.6181   Validation loss: 0.2670\n",
      "Epoch: 670   Training loss: 0.5784   Validation loss: 0.2644\n",
      "Epoch: 680   Training loss: 0.5863   Validation loss: 0.2617\n",
      "Epoch: 690   Training loss: 0.6081   Validation loss: 0.2591\n",
      "Epoch: 700   Training loss: 0.6419   Validation loss: 0.2568\n",
      "Epoch: 710   Training loss: 0.5985   Validation loss: 0.2542\n",
      "Epoch: 720   Training loss: 0.5974   Validation loss: 0.2517\n",
      "Epoch: 730   Training loss: 0.5989   Validation loss: 0.2492\n",
      "Epoch: 740   Training loss: 0.6013   Validation loss: 0.2473\n",
      "Epoch: 750   Training loss: 0.6283   Validation loss: 0.2453\n",
      "Epoch: 760   Training loss: 0.5579   Validation loss: 0.2434\n",
      "Epoch: 770   Training loss: 0.6046   Validation loss: 0.2414\n",
      "Epoch: 780   Training loss: 0.5933   Validation loss: 0.2396\n",
      "Epoch: 790   Training loss: 0.5484   Validation loss: 0.2380\n",
      "Epoch: 800   Training loss: 0.5534   Validation loss: 0.2360\n",
      "Epoch: 810   Training loss: 0.5958   Validation loss: 0.2342\n",
      "Epoch: 820   Training loss: 0.5717   Validation loss: 0.2327\n",
      "Epoch: 830   Training loss: 0.5840   Validation loss: 0.2311\n",
      "Epoch: 840   Training loss: 0.6127   Validation loss: 0.2298\n",
      "Epoch: 850   Training loss: 0.5891   Validation loss: 0.2281\n",
      "Epoch: 860   Training loss: 0.5969   Validation loss: 0.2264\n",
      "Epoch: 870   Training loss: 0.6017   Validation loss: 0.2252\n",
      "Epoch: 880   Training loss: 0.6011   Validation loss: 0.2236\n",
      "Epoch: 890   Training loss: 0.6364   Validation loss: 0.2220\n",
      "Epoch: 900   Training loss: 0.6006   Validation loss: 0.2208\n",
      "Epoch: 910   Training loss: 0.6035   Validation loss: 0.2199\n",
      "Epoch: 920   Training loss: 0.5500   Validation loss: 0.2187\n",
      "Epoch: 930   Training loss: 0.5898   Validation loss: 0.2178\n",
      "Epoch: 940   Training loss: 0.4917   Validation loss: 0.2169\n",
      "Epoch: 950   Training loss: 0.5513   Validation loss: 0.2158\n",
      "Epoch: 960   Training loss: 0.5562   Validation loss: 0.2142\n",
      "Epoch: 970   Training loss: 0.5354   Validation loss: 0.2129\n",
      "Epoch: 980   Training loss: 0.6046   Validation loss: 0.2118\n",
      "Epoch: 990   Training loss: 0.6084   Validation loss: 0.2110\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "train_validate_survival_model(model, train_dataloader, val_dataloader, loss_function, optimizer, num_epochs, patience, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "test_predictions = model(x_test).squeeze(-1)\n",
    "\n",
    "recurrent_predictions = test_predictions.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "nsamples = 100  # Number of bootstrap samples\n",
    "survival_cis = []\n",
    "recurrent_cis = []\n",
    "\n",
    "for sample in range(nsamples):\n",
    "    \n",
    "    # Resample the test data\n",
    "    x_test_resampled, t_test_resampled, e_test_resampled, d_test_resampled = resample(x_test, t_test, e_test, d_test, replace=True)\n",
    "    \n",
    "    # Make predictions on the resampled test data\n",
    "    test_predictions = model(x_test_resampled).squeeze(-1)\n",
    "    \n",
    "    survival_predictions = test_predictions.squeeze(-1)\n",
    "    try: \n",
    "        # Calculate metrics for the resampled test data\n",
    "        survival_cis.append(calculate_survival_metrics(survival_predictions, d_train, t_train, d_test_resampled, t_test_resampled))\n",
    "    except: \n",
    "        # When all samples are censored\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric_ci(cis, time_key, rounding = 2):\n",
    "    cis_time = [cis[i][time_key] for i in range(len(cis))]\n",
    "    survival_ci_mean = np.mean(cis_time)\n",
    "    survival_ci_std = np.std(np.array(cis_time))\n",
    "    return round(survival_ci_mean, rounding), round(survival_ci_std, rounding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 3: Mean = 0.96, Std = 0.02\n",
      "Time 4: Mean = 0.97, Std = 0.01\n",
      "Time 5: Mean = 0.97, Std = 0.01\n",
      "Time 6: Mean = 0.97, Std = 0.01\n"
     ]
    }
   ],
   "source": [
    "survival_cis_3_mean, survival_cis_3_std = compute_metric_ci(survival_cis, \"Time 3\")\n",
    "survival_cis_4_mean, survival_cis_4_std = compute_metric_ci(survival_cis, \"Time 4\")\n",
    "survival_cis_5_mean, survival_cis_5_std = compute_metric_ci(survival_cis, \"Time 5\")\n",
    "survival_cis_6_mean, survival_cis_6_std = compute_metric_ci(survival_cis, \"Time 6\")\n",
    "\n",
    "print(f\"Time 3: Mean = {survival_cis_3_mean}, Std = {survival_cis_3_std}\")\n",
    "print(f\"Time 4: Mean = {survival_cis_4_mean}, Std = {survival_cis_4_std}\")\n",
    "print(f\"Time 5: Mean = {survival_cis_5_mean}, Std = {survival_cis_5_std}\")\n",
    "print(f\"Time 6: Mean = {survival_cis_6_mean}, Std = {survival_cis_6_std}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
